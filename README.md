# Transformer

Checking how transformer sequence to sequence model works for NMT. With terrible way of training...

Transformer is described in:
Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems. 2017.

BPEmb encoder is described in:
Heinzerling, Benjamin, and Michael Strube. "BPEmb: Tokenization-free pre-trained subword embeddings in 275 languages." arXiv preprint arXiv:1710.02187 (2017).
